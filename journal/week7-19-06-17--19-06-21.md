# Week 7: 19-06-17--19-06-21

## **Plan for the week**

### 1) Finalizing the organization of the scrape data

- [ x ] Update the stats on the number of posts and comments per topic & totals with the scraped data
- [ x ] Transfer the scraped source html file on the hard drive
- [ x ] Run the script to parse the json files to csv files

### 2) Things to implement

- [ x ] Implement connection to Stanford CoreNLP for parsing sentences into grammar trees
  - [ - ] ParserAnnotator: https://stanfordnlp.github.io/CoreNLP/parse.html
- [ - ] Learn and implement Association Rule Mining -> Find most frequent pattern -> See Apriori algorithm
- [ - ] Combining the new approaches with keywords matching
- [ - ] Try to find a good approach to use the TF-IDF scores
  - [ - ] Have a look at: Finding The Most Important Sentences Using NLP & TF-IDF https://hackernoon.com/finding-the-most-important-sentences-using-nlp-tf-idf-3065028897a3

## 3) **Ideas to explore**

- [ - ] Build our own datasets for supervised learning for categorizing interrogative vs non interrogative sentences
- [ - ] Use title to get more info about the thread and distinguish the noisy data form the non-noisy data
- [ - ] Focus on question post and use the answer posts to bring more information
- [ - ] Find the need from the answer posts: find sentence structures that answer a need
- [ - ] Topic modeling for the whole conversation? 
  - For now we will only use the title and see if topic modeling could help later [14/06/2019] 
  - topic-modelling-with-spacy-and-scikit-learn: https://www.kaggle.com/thebrownviking20/topic-modelling-with-spacy-and-scikit-learn
- [ - ] Consider the position of the sentence.

## 4) **Things to research and learn about**:

- [ - ] Association rule mining -> Find most frequent pattern -> See Apriori algorithm etc.
- [ - ] How to reduce noise in forum conversation?
- [ - ] What techniques are used to optimize search result based on the search query
- [ - ] Find patterns in sentences that certainly do not express a need?
- [ - ] Reread CLiPS: https://www.clips.uantwerpen.be/pages/pattern-vector

## **Things to keep in mind**

- [ - ] Do we want to label some sentences: sentence that contains requirement vs sentence that does not contain requirement?
- [ - ] spaCy really good NLP library: https://spacy.io/
  - [ ~ ] spaCy vs NLTK: https://medium.com/@pemagrg/private-nltk-vs-spacy-3926b3674ee4
  
## Monday

- Transferred the scraped source html file on the hard drive
- Transferred folders "subforums" and "threads" on laptop
- Ran the script to parse the json files to csv files
- Updated the stats on the number of posts and comments per topic & totals with the scraped data
- Read about the
- Connected to Stanford Core NLP with NLTK

TODO :

- Take a sample of questions
- Parse the questions into sentences
- Parse the sentences with Standford Core NLP and retrieve the phrase-structure tree
- Read all the sentences and identify if they express a need or not
- Try to find a link between need expressed and phrase-structure tags fron Core-NLP 

### Stanford Core NLP

- The Stanford Parser: A statistical parser: https://nlp.stanford.edu/software/lex-parser.shtml
- Downloaded Stanford CoreNLP https://stanfordnlp.github.io/CoreNLP/download.html
- Online Parser: http://nlp.stanford.edu:8080/parser/
- Parser *Q&A*: https://nlp.stanford.edu/software/parser-faq.html#c  
- ParserAnnotator: https://stanfordnlp.github.io/CoreNLP/parse.html#shift-reduce-parser
- Shift-Reduce Constituency Parser: https://nlp.stanford.edu/software/srparser.html

#### Documentation about the tags used by Standford Core NLP

- Documents: https://catalog.ldc.upenn.edu/docs/LDC99T42/
- POS tagging guide: https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf
- Phrase structure bracketing guide: https://catalog.ldc.upenn.edu/docs/LDC99T42/prsguid1.pdf
- Penn Treebank II Constituent Tags (Other resource): http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html#SBAR
- THE PENN TREEBANK: AN OVERVIEW CHAP1 (Other resource): http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&rep=rep1&type=pdf

#### Stanford CoreNLP with NLTK

- Stanford CoreNLP API in NLTK: https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK
- In the folder ```stanford-corenlp-full-2018-10-05``` start the server with in the Command Prompt :
  - ```java -mx4g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer preload tokenize,ssplit,pos,lemma,ner,parse,depparse status_port 9000 -port 9000 -timeout 15000 &```
- In the Anaconda prompt execute script in the activated desired virtual environnement: 
  - ```scripts/core_nlp_test.py```

## Tuesday

## Wednesday

## Thursday

## Friday
