# Week 7: 19-06-17--19-06-21

## **Plan for the week**

### 1) Finalizing the organization of the scrape data

- [ x ] Update the stats on the number of posts and comments per topic & totals with the scraped data
- [ x ] Transfer the scraped source html file on the hard drive
- [ x ] Run the script to parse the json files to csv files

### 2) Things to implement

- [ x ] Implement connection to Stanford CoreNLP for parsing sentences into grammar trees
  - [ - ] ParserAnnotator: https://stanfordnlp.github.io/CoreNLP/parse.html
- [ - ] Learn and implement Association Rule Mining -> Find most frequent pattern -> See Apriori algorithm
- [ - ] Combining the new approaches with keywords matching
- [ - ] Try to find a good approach to use the TF-IDF scores
  - [ - ] Have a look at: Finding The Most Important Sentences Using NLP & TF-IDF https://hackernoon.com/finding-the-most-important-sentences-using-nlp-tf-idf-3065028897a3

## 3) **Ideas to explore**

- [ - ] Build our own datasets for supervised learning for categorizing interrogative vs non interrogative sentences
- [ - ] Use title to get more info about the thread and distinguish the noisy data form the non-noisy data
- [ - ] Focus on question post and use the answer posts to bring more information
- [ - ] Find the need from the answer posts: find sentence structures that answer a need
- [ - ] Topic modeling for the whole conversation? 
  - For now we will only use the title and see if topic modeling could help later [14/06/2019] 
  - topic-modelling-with-spacy-and-scikit-learn: https://www.kaggle.com/thebrownviking20/topic-modelling-with-spacy-and-scikit-learn
- [ - ] Consider the position of the sentence.

## 4) **Things to research and learn about**:

- [ - ] Association rule mining -> Find most frequent pattern -> See Apriori algorithm etc.
- [ - ] How to reduce noise in forum conversation?
- [ - ] What techniques are used to optimize search result based on the search query
- [ - ] Find patterns in sentences that certainly do not express a need?
- [ - ] Reread CLiPS: https://www.clips.uantwerpen.be/pages/pattern-vector

## **Things to keep in mind**

- [ x ] Do we want to label some sentences: sentence that contains requirement vs sentence that does not contain requirement?
- [ - ] spaCy really good NLP library: https://spacy.io/
  - [ ~ ] spaCy vs NLTK: https://medium.com/@pemagrg/private-nltk-vs-spacy-3926b3674ee4
  
## Monday

- Transferred the scraped source html file on the hard drive
- Transferred folders "subforums" and "threads" on laptop
- Ran the script to parse the json files to csv files
- Updated the stats on the number of posts and comments per topic & totals with the scraped data
- Read about Stanford Core NLP Parser
- Connected to Stanford Core NLP with NLTK

TODO :

- Take a sample of questions
- Parse the questions into sentences
- Parse the sentences with Standford Core NLP and retrieve the phrase-structure tree
- Read all the sentences and identify if they express a need or not
- Try to find a link between need expressed and phrase-structure tags from Core-NLP 

### Stanford Core NLP

- The Stanford Parser: A statistical parser: https://nlp.stanford.edu/software/lex-parser.shtml
- Downloaded Stanford CoreNLP https://stanfordnlp.github.io/CoreNLP/download.html
- Online Parser: http://nlp.stanford.edu:8080/parser/
- Parser *Q&A*: https://nlp.stanford.edu/software/parser-faq.html#c  
- ParserAnnotator: https://stanfordnlp.github.io/CoreNLP/parse.html#shift-reduce-parser
- Shift-Reduce Constituency Parser: https://nlp.stanford.edu/software/srparser.html

#### Documentation about the tags used by Standford Core NLP

- Documents: https://catalog.ldc.upenn.edu/docs/LDC99T42/
- POS tagging guide: https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf
- Phrase structure bracketing guide: https://catalog.ldc.upenn.edu/docs/LDC99T42/prsguid1.pdf
- Penn Treebank II Constituent Tags (Other resource): http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html#SBAR
- THE PENN TREEBANK: AN OVERVIEW CHAP1 (Other resource): http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&rep=rep1&type=pdf

#### Stanford CoreNLP with NLTK

- Stanford CoreNLP API in NLTK: https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK
- Documentation: http://www.nltk.org/_modules/nltk/parse/corenlp.html
- In the folder ```stanford-corenlp-full-2018-10-05``` start the server with in the Command Prompt :
  - ```java -mx4g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer preload tokenize,ssplit,pos,lemma,ner,parse,depparse status_port 9000 -port 9000 -timeout 15000 &```
- In the Anaconda prompt execute script in the activated desired virtual environnement: 
  - ```scripts/core_nlp_test.py```

## Tuesday

- Refactored project structure, it is now ready (clean and scalable)
- Had to work a bit with the data to have access to the information: which thread is part of which subforum (Problem solved)
- Added new threads data to all backups
- Wrote pseudo code for algo to retrieve 2 % of the threads
TODO:
- Retrieve the 2% of threads data
- Adjust/confirm the keywords for finding requirements sentences
- Categorize the data in the 3 categories (1: Interrogative sentences, 2: Keywords matching sentences, 3: Other sentences not part of 1 & 2)
- Find how we are going to present the data to the workers

### Stanford CoreNLP 

#### Parsing Stanford CoreNLP trees

- Stanford CoreNLP parse tree format: https://stackoverflow.com/questions/34395127/stanford-nlp-parse-tree-format 
- Stanford CoreNLP Tree documentation: https://nlp.stanford.edu/nlp/javadoc/javanlp-3.5.0/edu/stanford/nlp/trees/Tree.html
- The NLTK tree's documentation helps to understand how to parse the trees
  - There are libraries out there to read bracketed parse, e.g. in NLTK's nltk.tree.Tree: http://www.nltk.org/howto/tree.html
  - Source code for nltk.tree (API): https://www.nltk.org/_modules/nltk/tree.html
  - NLTK How to: http://www.nltk.org/howto/tree.html
  - A notebook playing with these methods: https://nbviewer.jupyter.org/github/gmonce/nltk_parsing/blob/master/1.%20NLTK%20Syntax%20Trees.ipynb
  
## Wednesday

- Retrieved 2% of threads data
- Wrote scripts to: 
  - Get a sample of threads (get_percentage_of_threads_data.py)
  - Parse the posts of the threads to sentences (parse_posts_to_sentences.py)
  - Remove the undesired sentences (remove_short_messages.py)

TODO: 

- Select a subset of the parsed_0.02_of_threads_to_sentences data
- Adjust/confirm the keywords for finding requirements sentences
- Categorize the data in the 3 categories (1: Interrogative sentences, 2: Keywords matching sentences, 3: Other sentences not part of 1 & 2)
- Find the best format to present the data to the workers

## Thursday

- Fixed unwanted data in the datasets
- Added new annotations and found their synonyms
- Wrote script to tag if sentence is a question with coreNLP (solved problem with high number of request)
- Wrote script to tag if sentence has annotation word
- I`ll run the scripts this evening (I started running them this afternoon but an unexpected problem occurred so I have to redo it^^)
TODO: 
- Finish running the scripts 
- Select a subset of data for the workers
- Find the best format to present the data to the workers
  
## Friday

- Parsed sentences with CoreNLP Tagger
- Parsed sentences with Annotation Tagger
- Created parsed datasets
  - Dataset #1: All question sentences
  - Dataset #2: Sentences with annotations that are not in question form
  - Dataset #3: All other sentences
- Extract statistics from the datasets

TODO: 

- Sample some sentences
- Prepare two sets for amazon mechanical turk
  - 1: Interrogative sentences
  - 2: Keywords + non keywords sentences
- Submit to Amazon
- Analyse data and find if first posts of each threads have more information
- Work & read on extractive summary
  - Extractive summary of whole threads?
  - Extractive summary of each comments?
