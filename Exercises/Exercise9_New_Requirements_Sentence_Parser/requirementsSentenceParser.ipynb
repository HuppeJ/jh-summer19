{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\n# ms-python.python added\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..\\\\..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["# Extracting Sentences Containing Requirements Keywords V2\n","## Algorithm:\n"," - 1. Open question database csv file\n"," - 2. Split question into sentences\n"," - 3. Clean sentences text\n","   - 3.1 Remove links\n","   - 3.2 Remove @ references\n","   - 3.3 Only keep characters matching\n","   - 3.4 Lower all characters\n"," - 4. Get the requirement words from csv file\n"," - 5. Find the requirements sentences based on the requirement words\n","   - 5.1 Possible TODO: We could check if lemmatizing or stemming the words in the sentences\n","       before checking if they are part of the requirement words would help\n"," - 6. Remove stop words from  requirements sentences\n"," - 7.  Get the lemmatized words from the remaining words\n"," - 8.  Write in csv file new values"],"metadata":{}},{"source":["import csv\n","import os\n","import re\n","from nltk import sent_tokenize\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"source":["# Init tools\n","stop_words = stopwords.words('english')\n","porter = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":2},{"source":["# Open question database csv file\n","dir_path = r\"C:\\Users\\jerem\\Desktop\\jh-summer19\\Exercises\\Exercise9_New_Requirements_Sentence_Parser\"\n","input_file_name = \"\\questions_db.csv\"\n","#input_file_name = \"\\questions_db_sample.csv\"\n","\n","with open(dir_path + input_file_name) as csv_file:\n","    csv_reader = csv.reader(csv_file, delimiter=',')\n","\n","    # Creating a dictionary with all the questions\n","    # data_dict = {\"thread_id\": question } \n","    for row in csv_reader:\n","        data_dict = {rows[0]:rows[6] for rows in csv_reader}\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":3},{"source":["# Split question into sentences\n","# data_dict = {\"thread_id\": {\"question\": question, \"sentences\": sentences}} \n","for k in data_dict:\n","    sentences = sent_tokenize(data_dict[k])\n","    for i, sentence in enumerate(sentences):\n","        # Clean sentences text\n","        sentences[i] = re.sub(r\"https?://\\S+\", \"\" , sentences[i])\n","        sentences[i] = re.sub(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \" , sentences[i])\n","        sentences[i] = re.sub(r\"@\", \"at\" , sentences[i])\n","        sentences[i] = sentences[i].lower()\n","    question = data_dict[k]\n","    data_dict[k] = {\n","        \"question\": question, \n","        \"sentences\": sentences\n","    }\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":4},{"source":["# Get the requirement words from csv file\n","requirement_words = []\n","annotation_file_name = r\"\\improved_annotations.csv\"\n","\n","requirements_word_dict = {}\n","with open(dir_path + annotation_file_name) as csv_file:\n","    csv_reader = csv.reader(csv_file, delimiter=',')\n","    requirements_word_dict = {rows[0] for rows in csv_reader}\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":5},{"source":["# Find the requirements sentences based on the requirement words\n","requirements_sentences = []\n","for k in data_dict:\n","    for sentence in data_dict[k][\"sentences\"]:\n","        # TODO: We could check if lemmatizing or stemming the word would help  \n","        if any(word in sentence for word in requirements_word_dict):\n","            requirements_sentences.append(sentence)\n","    question = data_dict[k][\"question\"]\n","    sentences = data_dict[k][\"sentences\"]\n","    data_dict[k] = {\n","        \"question\": question, \n","        \"sentences\": sentences, \n","        \"requirements_sentences\": requirements_sentences\n","    }\n","    requirements_sentences = []\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":6},{"source":["# Remove stop words from requirements sentences\n","for k in data_dict:\n","    lemmatized_words = []    \n","    words = []\n","\n","    for requirements_sentence in data_dict[k][\"requirements_sentences\"]:\n","        tokenned_words = word_tokenize(requirements_sentence)\n","        cleanned_words = [word for word in tokenned_words if word not in stop_words and word.isalpha()]\n","        words.extend(cleanned_words)\n","        # Get the lemmatized words from the remaining words\n","        for word in words:\n","            lemmatized_words.append(lemmatizer.lemmatize(word))\n","    \n","    question = data_dict[k][\"question\"]\n","    sentences = data_dict[k][\"sentences\"]\n","    requirements_sentences = data_dict[k][\"requirements_sentences\"]\n","\n","    data_dict[k] = {\n","        \"question\": question, \n","        \"sentences\": sentences, \n","        \"requirements_sentences\": requirements_sentences, \n","        \"words\": words,\n","        \"lemmatized_words\": list(dict.fromkeys(lemmatized_words)) \n","    }\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":7},{"source":["# Write in csv file new values\n","output_file_name = \"\\questions_db_parsed.csv\"\n","#output_file_name = \"\\questions_db_sample_parsed.csv\"\n","\n","with open(dir_path + output_file_name, mode='w', encoding=\"utf-8\", newline='') as csv_file:\n","    csv_writer = csv.writer(csv_file, delimiter=',')\n","\n","    csv_writer.writerow([\"thread-id\", \"question\", \"sentences\", \"requirements_sentences\", \"words\", \"lemmatized_words\"])\n","\n","    for k in data_dict:\n","        csv_writer.writerow([k, data_dict[k][\"question\"], data_dict[k][\"sentences\"], data_dict[k][\"requirements_sentences\"], data_dict[k][\"words\"], data_dict[k][\"lemmatized_words\"]])\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":8},{"source":["\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":9}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}