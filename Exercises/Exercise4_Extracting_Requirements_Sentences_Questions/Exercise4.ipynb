{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..\\..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["# Extracting Sentences Containing Requirements Keywords"],"metadata":{}},{"source":["import csv\n","import os\n","from nltk import sent_tokenize\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"source":["# Init tools\n","stop_words = stopwords.words('english')\n","porter = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":2},{"source":["# Open question database csv file\n","dir_path = r\"C:\\Users\\jerem\\Desktop\\jh-summer19\\Exercises\\Exercise4_Extracting_Requirements_Sentences_Questions\"\n","input_file_name = \"\\questions_db.csv\"\n","# input_file_name = \"\\questions_db_sample.csv\"\n","\n","with open(dir_path + input_file_name) as csv_file:\n","    csv_reader = csv.reader(csv_file, delimiter=',')\n","\n","    # Creating a dictionary with all the questions\n","    # data_dict = {\"thread_id\": question } \n","    for row in csv_reader:\n","        data_dict = {rows[0]:rows[6] for rows in csv_reader}\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":3},{"source":["# Adding attribute sentences to data_dict\n","# data_dict = {\"thread_id\": {\"question\": question, \"sentences\": sentences}} \n","for k in data_dict:\n","    # split into sentences\n","    sentences = sent_tokenize(data_dict[k])\n","    for i, sentence in enumerate(sentences):\n","        sentences[i] = sentence.lower()\n","    question = data_dict[k]\n","    data_dict[k] = {\n","        \"question\": question, \n","        \"sentences\": sentences\n","    }\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":4},{"source":["# Finding requirements sentences\n","requirements_word_dict = {\"need\", \"how\", \"i will be interested\", \"can't find\", \"i am trying\", \"help\"}\n","requirements_sentences = []\n","for k in data_dict:\n","    for sentence in data_dict[k][\"sentences\"]:\n","        if any(word in sentence for word in requirements_word_dict):\n","            requirements_sentences.append(sentence)\n","    question = data_dict[k][\"question\"]\n","    sentences = data_dict[k][\"sentences\"]\n","    data_dict[k] = {\n","        \"question\": question, \n","        \"sentences\": sentences, \n","        \"requirements_sentences\": requirements_sentences\n","    }\n","    requirements_sentences = []\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":5},{"source":["\n","# Clean requirements_sentences's words\n","for k in data_dict:\n","    lemmatized_words = []    \n","    words = []\n","\n","    for requirements_sentence in data_dict[k][\"requirements_sentences\"]:\n","        tokenned_words = word_tokenize(requirements_sentence)\n","        cleanned_words = [word for word in tokenned_words if word not in stop_words and word.isalpha()]\n","        words.extend(cleanned_words)\n","        for word in words:\n","            # print(porter.stem(word))\n","            # print(lemmatizer.lemmatize(word))\n","            lemmatized_words.append(lemmatizer.lemmatize(word))\n","    \n","    question = data_dict[k][\"question\"]\n","    sentences = data_dict[k][\"sentences\"]\n","    requirements_sentences = data_dict[k][\"requirements_sentences\"]\n","\n","    data_dict[k] = {\n","        \"question\": question, \n","        \"sentences\": sentences, \n","        \"requirements_sentences\": requirements_sentences, \n","        \"words\": words,\n","        \"lemmatized_words\": lemmatized_words \n","    }\n","\n","\n","\n","#print(lemmatized_words)"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":6},{"source":["# Write in csv file new values\n","output_file_name = \"\\questions_db_parsed.csv\"\n","# output_file_name = \"\\questions_db_sample_parsed.csv\"\n","\n","with open(dir_path + output_file_name, mode='w', encoding=\"utf-8\", newline='') as csv_file:\n","    csv_writer = csv.writer(csv_file, delimiter=',')\n","\n","    csv_writer.writerow([\"thread-id\", \"question\", \"sentences\", \"requirements_sentences\", \"words\", \"lemmatized_words\"])\n","\n","    for k in data_dict:\n","        csv_writer.writerow([k, data_dict[k][\"question\"], data_dict[k][\"sentences\"], data_dict[k][\"requirements_sentences\"], data_dict[k][\"words\"], data_dict[k][\"lemmatized_words\"]])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":7},{"source":[""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":8}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}